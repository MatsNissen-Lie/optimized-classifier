{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calculate_error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization and Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Important Equations\n",
    "\n",
    "$$\n",
    "C_{w_0, w}(x) = \\text{sign}(w_0 + x^T w) \\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$\n",
    "\\text{minimize}_{w_0, w} \\, \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}_{\\mathbb{R}_-} \\left( y_n C_{w_0, w}(x_n) \\right), \\tag{2}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{1}_{\\mathbb{R}_-}(u) = \\begin{cases} \n",
    "1, & \\text{if } u < 0 \\\\\n",
    "0, & \\text{if } u \\geq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$\n",
    "\\mathbf{1}_{\\mathbb{R}_-} \\left( y_n C_{w_0, w}(x_n) \\right) = \\mathbf{1}_{\\mathbb{R}_-} \\left( y_n (w_0 + x_n^T w) \\right); \\tag{3}\n",
    "$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$\n",
    "\\text{minimize}_{w_0, w} \\, \\frac{1}{N} \\sum_{n=1}^{N} h \\left( y_n (w_0 + x_n^T w) \\right) \\tag{4}\n",
    "$$\n",
    "where\n",
    "\n",
    "$$\n",
    "h(u) = (1 - u)_+.\n",
    "$$\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$\n",
    "\\text{minimize}_{w_0, w} \\, \\frac{1}{N} \\sum_{n=1}^{N} h \\left( y_n (w_0 + x_n^T w) \\right) + \\rho \\| w \\|_2^2, \\tag{5}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "g_D(w_0, w) = \\frac{1}{N} \\sum_{n=1}^{N} h \\left( y_n (w_0 + x_n^T w) \\right)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "r(w_0, w) = \\rho \\| w \\|_2^2,\n",
    "$$\n",
    "\n",
    "so that the total cost function is:\n",
    "\n",
    "$$\n",
    "g(w_0, w) = g_D(w_0, w) + r(w_0, w).\n",
    "$$\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "$$\n",
    "\\text{minimize}_{w_0, w} \\ \\frac{1}{N} \\sum_{n=1}^{N} h \\left( y_n \\left( w_0 + x_n^T w \\right) - P \\| y_n w \\|_1 \\right) + \\rho \\| w \\|_2^2, \\tag{9}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "### Context\n",
    "\n",
    "The Matlab file classifier_dataset.mat in the project root contains the following four variables: \n",
    "- **traindataset**: This is a matrix of size $N \\times D$, where $N = 400$ and $D = 784$. The $n$-th row of this matrix corresponds to $x_n^T$ in (5). We obtained each such $x_n$ by flattening a $28 \\times 28$ image from the MNIST dataset, which contains images of handwritten digits. We consider only images of the digits 0 and 1.\n",
    "\n",
    "- **trainlabels**: This is a vector of length $N$. The $n$-th component of this vector corresponds to $y_n$ in (5) and indicates the category of the $n$-th row of the matrix **traindataset**. (We chose to represent the category of digit 0 as $y_n = -1$ and the category of digit 1 as $y_n = 1$.)\n",
    "\n",
    "- **testdataset**: This is a matrix of size 1600 $\\times$ 784, each row containing a flattened image of a handwritten digit 0 or 1. The images in this matrix are different from those included in **traindataset**. This matrix will be used to validate the extrapolation capacity of our classifiers.\n",
    "\n",
    "- **testlabels**: This is a vector of length 1600. The $n$-th component of this vector indicates the category of the $n$-th row of the matrix **testdataset**.\n",
    "\n",
    "Recall equation (5):\n",
    "$$\n",
    "\\text{minimize}_{w_0, w} \\, \\frac{1}{N} \\sum_{n=1}^{N} h \\left( y_n (w_0 + x_n^T w) \\right) + \\rho \\| w \\|_2^2, \\tag{5}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "h(u) = (1 - u)_+.\n",
    "$$\n",
    "\n",
    "### Task\n",
    "Use CVX to solve problem (5) where $x_n$ and $y_n$ denote the $n$-th row of the matrix **traindataset** and the $n$-th component of the vector **trainlabels**, respectively. Use $\\rho = 0.1$.\n",
    "\n",
    "With the parameters $(w_0, w)$ thus obtained, assess the performance of the corresponding classifier $C_{w_0, w}$ defined in (1) on the training dataset and the test dataset: specifically, evaluate the function $f_D$ defined in (2) by first considering that $(x_n, y_n)$ come from **traindataset** and **trainlabels**, which gives the classifier error rate on the training dataset; next, consider that $(x_n, y_n)$ come from **testdataset** and **testlabels**, which gives the classifier error rate on the test dataset.\n",
    "\n",
    "So that you can check your code, we now give such values of $f_D$ for the case $\\rho = 0.5$: for the training dataset, $f_D$ evaluates to approximately 0.25%, and, for the test dataset, $f_D$ evaluates to 0.25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error rate: 0.00%\n",
      "Test error rate: 0.12%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Load the dataset\n",
    "data = loadmat('data/classifier_dataset.mat')\n",
    "\n",
    "# Extract variables\n",
    "X_train = data['traindataset']     # Training data (400 x 784)\n",
    "y_train = data['trainlabels'].flatten()  # Training labels (400,)\n",
    "N_train, D = X_train.shape\n",
    "\n",
    "X_test = data['testdataset']       # Test data (1600 x 784)\n",
    "y_test = data['testlabels'].flatten()    # Test labels (1600,)\n",
    "N_test = X_test.shape[0]\n",
    "\n",
    "# Regularization parameter\n",
    "rho = 0.1\n",
    "# rho = 0.5\n",
    "\n",
    "# Define variables for optimization\n",
    "w0 = cp.Variable()  # Scalar bias term\n",
    "w = cp.Variable(D)  # Weight vector of size D\n",
    "\n",
    "# Define the hinge loss function\n",
    "u = cp.multiply(y_train, w0 + X_train @ w)  # y_n * (w0 + x_n.T @ w)\n",
    "loss = (1 / N_train) * cp.sum(cp.pos(1 - u))  # Hinge loss\n",
    "reg = rho * cp.norm(w, 2)**2  # Regularization term\n",
    "objective = cp.Minimize(loss + reg)\n",
    "\n",
    "# Solve the optimization problem\n",
    "problem = cp.Problem(objective)\n",
    "problem.solve()\n",
    "\n",
    "# Extract optimal parameters\n",
    "w0_value = w0.value\n",
    "w_value = w.value\n",
    "\n",
    "# Compute predictions and error rate on training data\n",
    "scores_train = w0_value + X_train @ w_value  # Scores for training data\n",
    "y_pred_train = np.sign(scores_train)         # Predicted labels for training data\n",
    "\n",
    "# Handle the case when score is zero (assign label 1 when score is zero)\n",
    "y_pred_train[scores_train == 0] = 1\n",
    "\n",
    "\n",
    "calculate_error_rate(y_pred_train, y_train, \"Training\")\n",
    "\n",
    "scores_test = w0_value + X_test @ w_value    # Scores for test data\n",
    "y_pred_test = np.sign(scores_test)           # Predicted labels for test data\n",
    "\n",
    "# Handle the case when score is zero (assign label 1 when score is zero)\n",
    "y_pred_test[scores_test == 0] = 1\n",
    "\n",
    "\n",
    "calculate_error_rate(y_pred_test, y_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_im(x):\n",
    "    # Reshape the flattened image back to 28x28\n",
    "    image = x.reshape(28, 28).T  # Transpose to match the original orientation\n",
    "    \n",
    "    # Rescale the image data to [0, 255]\n",
    "    min_val = np.min(image)\n",
    "    max_val = np.max(image)\n",
    "    image_rescaled = (image - min_val) / (max_val - min_val) * 255\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image_rescaled, cmap='gray', vmin=0, vmax=255)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# Example usage\n",
    "sample_index = 1  # Change this index to view different images\n",
    "show_im(X_test[sample_index])\n",
    "print(f'Label: {y_test[sample_index]}')  # -1 for digit 0, 1 for digit 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 7 [Theoretical Task]**\n",
    "\n",
    "### **Problem Statement**\n",
    "\n",
    "An attacker aims to manipulate a feature vector $ x $ before it is classified by a linear classifier $ C_{w_0,w}(x) = \\text{sign}(w_0 + w^T x) $. The attacker knows the classifier's parameters $(w_0, w)$, the true label $ y $ of $ x $, and can perturb each component of $ x $ within a bounded range $ P $.\n",
    "\n",
    "**Optimization Problem (Equation 8):**\n",
    "$$\n",
    "\\min_{\\tilde{x}} \\quad y(w_0 + \\tilde{x}^T w) \\\\\n",
    "\\text{subject to} \\quad |\\tilde{x}_d - x_d| \\leq P, \\quad \\forall d = 1, 2, \\dots, D\n",
    "$$\n",
    "\n",
    "Here, $ y \\in \\{-1, 1\\} $ represents the true class label, and the goal is to induce a misclassification by minimizing the decision function under perturbation constraints.\n",
    "\n",
    "---\n",
    "\n",
    "### **Part (a): Optimal Perturbation Vector $\\tilde{x}$**\n",
    "\n",
    "**Objective:**  \n",
    "Show that the vector transformation $\\tilde{x} = x - P \\cdot \\text{sgn}(y w)$ solves the optimization problem in Equation (8), where $\\text{sgn}(\\cdot)$ is applied component-wise.\n",
    "\n",
    "#### **Solution:**\n",
    "\n",
    "1. **Understanding the Objective Function:**\n",
    "   \n",
    "   The attacker seeks to minimize:\n",
    "   $$\n",
    "   y(w_0 + \\tilde{x}^T w)\n",
    "   $$\n",
    "   \n",
    "   Since $ y \\in \\{-1, 1\\} $, the product $ y w $ adjusts the direction of the weight vector based on the true label. Specifically:\n",
    "   - If $ y = 1 $, the goal is to decrease $ w_0 + \\tilde{x}^T w $.\n",
    "   - If $ y = -1 $, the goal is to increase $ w_0 + \\tilde{x}^T w $.\n",
    "\n",
    "2. **Formulating the Perturbation:**\n",
    "   \n",
    "   The attacker can perturb each component $ x_d $ within $ \\pm P $. To minimize $ y(w_0 + \\tilde{x}^T w) $, the attacker should adjust each $ \\tilde{x}_d $ in the direction that most negatively impacts the decision function.\n",
    "\n",
    "3. **Determining the Direction of Perturbation:**\n",
    "   \n",
    "   - **Case 1:** $ y = 1 $\n",
    "     - Minimize $ w_0 + \\tilde{x}^T w $.\n",
    "     - To minimize, decrease $ \\tilde{x}^T w $.\n",
    "     - Optimal perturbation: $ \\tilde{x}_d = x_d - P \\cdot \\text{sgn}(w_d) $.\n",
    "   \n",
    "   - **Case 2:** $ y = -1 $\n",
    "     - Minimize $ - (w_0 + \\tilde{x}^T w) $ $ \\Rightarrow $ Maximize $ w_0 + \\tilde{x}^T w $.\n",
    "     - To maximize, increase $ \\tilde{x}^T w $.\n",
    "     - Optimal perturbation: $ \\tilde{x}_d = x_d + P \\cdot \\text{sgn}(w_d) $.\n",
    "\n",
    "4. **Unified Expression Using $ y $:**\n",
    "   \n",
    "   Observe that both cases can be combined into a single expression by factoring in $ y $:\n",
    "   $$\n",
    "   \\tilde{x}_d = x_d - P \\cdot y \\cdot \\text{sgn}(w_d)\n",
    "   $$\n",
    "   \n",
    "   Or equivalently:\n",
    "   $$\n",
    "   \\tilde{x} = x - P \\cdot y \\cdot \\text{sgn}(w)\n",
    "   $$\n",
    "   \n",
    "   However, the problem statement provides:\n",
    "   $$\n",
    "   \\tilde{x} = x - P \\cdot \\text{sgn}(y w)\n",
    "   $$\n",
    "   \n",
    "   Since $ \\text{sgn}(y w_d) = y \\cdot \\text{sgn}(w_d) $ (because $ y $ is either $ 1 $ or $ -1 $), we can write:\n",
    "   $$\n",
    "   \\text{sgn}(y w_d) = y \\cdot \\text{sgn}(w_d)\n",
    "   $$\n",
    "   \n",
    "   Therefore:\n",
    "   $$\n",
    "   \\tilde{x} = x - P \\cdot y \\cdot \\text{sgn}(w) = x - P \\cdot \\text{sgn}(y w)\n",
    "   $$\n",
    "   \n",
    "   This shows that the proposed transformation $\\tilde{x} = x - P \\cdot \\text{sgn}(y w)$ is indeed the optimal solution to the optimization problem.\n",
    "\n",
    "#### **Conclusion:**\n",
    "The perturbation vector $\\tilde{x} = x - P \\cdot \\text{sgn}(y w)$ minimizes the objective function $ y(w_0 + \\tilde{x}^T w) $ under the given constraints, thereby solving the optimization problem outlined in Equation (8).\n",
    "\n",
    "---\n",
    "\n",
    "### **Part (b): Evaluating the Cost Function with $\\tilde{x}$**\n",
    "\n",
    "**Objective:**  \n",
    "Show that for the vector $ \\tilde{x} = x - P \\cdot \\text{sgn}(y w) $, the cost function evaluates to:\n",
    "$$\n",
    "y(w_0 + x^T w) - P \\| y w \\|_1\n",
    "$$\n",
    "where $ \\| \\cdot \\|_1 $ denotes the $ \\ell_1 $-norm.\n",
    "\n",
    "#### **Solution:**\n",
    "\n",
    "1. **Substituting $\\tilde{x}$ into the Cost Function:**\n",
    "   \n",
    "   The cost function to evaluate is:\n",
    "   $$\n",
    "   y(w_0 + \\tilde{x}^T w)\n",
    "   $$\n",
    "   \n",
    "   Substitute $ \\tilde{x} = x - P \\cdot \\text{sgn}(y w) $:\n",
    "   $$\n",
    "   y \\left( w_0 + (x - P \\cdot \\text{sgn}(y w))^T w \\right )\n",
    "   $$\n",
    "   \n",
    "   Expand the expression:\n",
    "   $$\n",
    "   y(w_0 + x^T w - P \\cdot (\\text{sgn}(y w))^T w)\n",
    "   $$\n",
    "\n",
    "2. **Simplifying the Perturbation Term:**\n",
    "   \n",
    "   - The term $ (\\text{sgn}(y w))^T w $ can be interpreted component-wise:\n",
    "     $$\n",
    "     (\\text{sgn}(y w))^T w = \\sum_{d=1}^{D} \\text{sgn}(y w_d) \\cdot w_d\n",
    "     $$\n",
    "   \n",
    "   - Notice that:\n",
    "     $$\n",
    "     \\text{sgn}(y w_d) \\cdot w_d = \\text{sgn}(y w_d) \\cdot |w_d| \\cdot \\text{sgn}(w_d) = y \\cdot |w_d|\n",
    "     $$\n",
    "     (Since $ \\text{sgn}(y w_d) = y \\cdot \\text{sgn}(w_d) $)\n",
    "   \n",
    "   - Therefore:\n",
    "     $$\n",
    "     (\\text{sgn}(y w))^T w = y \\cdot \\sum_{d=1}^{D} |w_d| = y \\| w \\|_1\n",
    "     $$\n",
    "\n",
    "3. **Substituting Back into the Cost Function:**\n",
    "   \n",
    "   $$\n",
    "   y(w_0 + x^T w - P \\cdot y \\| w \\|_1) = y w_0 + y x^T w - P y^2 \\| w \\|_1\n",
    "   $$\n",
    "   \n",
    "   Since $ y^2 = 1 $ (because $ y \\in \\{-1, 1\\} $):\n",
    "   $$\n",
    "   = y w_0 + y x^T w - P \\| w \\|_1\n",
    "   $$\n",
    "\n",
    "   Factor out $ y $ from the first two terms:\n",
    "   $$\n",
    "   = y (w_0 + x^T w) - P \\| w \\|_1\n",
    "   $$\n",
    "\n",
    "   Recognizing that $ \\| y w \\|_1 = | y | \\cdot \\| w \\|_1 = \\| w \\|_1 $ (since $ | y | = 1 $):\n",
    "   $$\n",
    "   = y (w_0 + x^T w) - P \\| y w \\|_1\n",
    "   $$\n",
    "\n",
    "4. **Final Expression:**\n",
    "   \n",
    "   Thus, the cost function evaluates to:\n",
    "   $$\n",
    "   y(w_0 + x^T w) - P \\| y w \\|_1\n",
    "   $$\n",
    "\n",
    "#### **Conclusion:**\n",
    "For the perturbation $ \\tilde{x} = x - P \\cdot \\text{sgn}(y w) $, the cost function $ y(w_0 + \\tilde{x}^T w) $ simplifies to:\n",
    "$$\n",
    "y(w_0 + x^T w) - P \\| y w \\|_1\n",
    "$$\n",
    "This demonstrates how the optimal adversarial perturbation affects the classifier's decision function by reducing it by a term proportional to the $ \\ell_1 $-norm of $ y w $.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- **Part (a):\n",
    "\n",
    "**  \n",
    "  The optimal perturbation $ \\tilde{x} = x - P \\cdot \\text{sgn}(y w) $ minimizes the adversarial objective $ y(w_0 + \\tilde{x}^T w) $ under bounded perturbation constraints.\n",
    "\n",
    "- **Part (b):**  \n",
    "  Substituting $ \\tilde{x} $ into the cost function yields $ y(w_0 + x^T w) - P \\| y w \\|_1 $, illustrating the impact of the adversarial attack on the classifier's decision.\n",
    "\n",
    "These results highlight the vulnerability of linear classifiers to adversarial perturbations and quantify the maximum degradation in the decision function achievable within the specified perturbation bounds.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. [Numerical task] \n",
    "From now on, take P = 0.18. Using the result of Task 7, attack the test dataset by replacing each example (xn,yn) in that dataset with the corresponding attacked example (xen,yn). Then, assess the performance of the classifier from Task 6 in this attacked dataset: specifically, evaluate the function fD defined in (2) but with (xn,yn) replaced by (xen,yn), which gives the classifier error rate on the attacked test dataset.\n",
    "So that you can check your code, we now give such value of fD for the case ρ = 0.5: for the attacked test dataset, fD evaluates to 21.9%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Test error rate: 43.56%\n"
     ]
    }
   ],
   "source": [
    "## Adversarial Attack Implementation\n",
    "\n",
    "def compute_perturbation(X, y, w, P):\n",
    "    \"\"\"\n",
    "    Computes the optimal perturbation for a set of inputs.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input data matrix (N x D)\n",
    "    - y: True labels vector (N,)\n",
    "    - w: Weight vector of the classifier (D,)\n",
    "    - P: Perturbation bound (scalar)\n",
    "\n",
    "    Returns:\n",
    "    - X_adv: Adversarially perturbed data matrix (N x D)\n",
    "    \"\"\"\n",
    "    # Compute sign(y * w) for each feature\n",
    "    sign_yw = np.sign(y[:, np.newaxis] * w[np.newaxis, :])  # Shape: (N, D)\n",
    "    \n",
    "    # Compute perturbation\n",
    "    perturbation = P * sign_yw  # Shape: (N, D)\n",
    "    \n",
    "    # Apply perturbation: x̃ = x - P * sign(yw)\n",
    "    X_adv = X - perturbation\n",
    "    \n",
    "    # Ensure perturbations are within [-P, P]\n",
    "    X_adv = np.clip(X_adv, X - P, X + P)\n",
    "    \n",
    "    return X_adv\n",
    "\n",
    "# Define perturbation magnitude P\n",
    "P = 0.18  # Adjust as needed\n",
    "\n",
    "# Generate adversarial examples for the test set\n",
    "X_test_adv = compute_perturbation(X_test, y_test, w_value, P)\n",
    "\n",
    "# Compute predictions on adversarial test data\n",
    "scores_test_adv = w0_value + X_test_adv @ w_value\n",
    "y_pred_test_adv = np.sign(scores_test_adv)\n",
    "y_pred_test_adv[scores_test_adv == 0] = 1\n",
    "\n",
    "# Calculate error rate on adversarial test data\n",
    "calculate_error_rate(y_pred_test_adv, y_test, \"Adversarial Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. [Numerical task] \n",
    "Recall: \n",
    "$$\n",
    "\\text{minimize}_{w_0, w} \\, \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}_{\\mathbb{R}_-} \\left( y_n C_{w_0, w}(x_n) \\right), \\tag{2}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\text{minimize}_{w_0, w} \\ \\frac{1}{N} \\sum_{n=1}^{N} h \\left( y_n \\left( w_0 + x_n^T w \\right) - P \\| y_n w \\|_1 \\right) + \\rho \\| w \\|_2^2, \\tag{9}\n",
    "$$\n",
    "\n",
    "\n",
    "Use CVX to solve problem (9) where xn and yn denote the nth row of the matrix traindataset and the nth component of the vector trainlabels, respectively. Use ρ = 0.1.\n",
    "With the parameters (w0, w) thus obtained, assess the performance of the corresponding clas- sifier Cw0,w defined in (1) on the training dataset and original test dataset: specifically, evalu- ate the function fD defined in (2) by first considering that (xn, yn) come from traindataset and trainlabels, which gives the classifier error rate on the training dataset; next, con- sider that (xn, yn) come from testdataset and testlabels, which gives the classifier error rate on the test dataset. Finally, and more importantly, assess the performance of this new classifier in the attacked dataset that you generated in Task 8.\n",
    "Comment these results by comparing them with the results you obtained in Tasks 6 and 8.\n",
    "So that you can check your code, we now give such values of fD for the case ρ = 0.5: for the training dataset, fD evaluates to approximately 0.25%, for the test dataset, fD evaluates to 0.19%, and for the attacked test dataset, fD evaluates to 3.69%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error rate: 0.75%\n",
      "Test error rate: 0.44%\n",
      "Adversarial Test error rate: 2.19%\n"
     ]
    }
   ],
   "source": [
    "# Regularization parameter\n",
    "rho = 0.1\n",
    "\n",
    "# Perturbation bound\n",
    "P = 0.18  # You can adjust this value as needed\n",
    "\n",
    "# Define variables for optimization\n",
    "w0 = cp.Variable()      # Scalar bias term\n",
    "w = cp.Variable(D)      # Weight vector of size D\n",
    "\n",
    "# Compute ||w||_1 once, since it's the same for all samples\n",
    "w_l1 = cp.norm1(w)\n",
    "\n",
    "\n",
    "# Define the adjusted hinge loss\n",
    "# For each sample: h(y_n (w0 + x_n^T w) - P ||w||_1), note that ||w*y||_1 = ||w||_1 because y = {-1, 1}\n",
    "u = cp.multiply(y_train, w0 + X_train @ w) - P * w_l1\n",
    "#  y_train * (w0 + X_train @ w) - P * w_l1  # Shape: (N_train,)\n",
    "\n",
    "# Hinge loss: max(0, 1 - u)\n",
    "loss = (1 / N_train) * cp.sum(cp.pos(1 - u))\n",
    "\n",
    "# Regularization term\n",
    "reg = rho * cp.norm(w, 2)**2  # L2 regularization\n",
    "\n",
    "# Objective function\n",
    "objective = cp.Minimize(loss + reg)\n",
    "\n",
    "# Define and solve the problem\n",
    "problem = cp.Problem(objective)\n",
    "problem.solve()\n",
    "\n",
    "# Extract optimal parameters\n",
    "w0_value = w0.value\n",
    "w_value = w.value\n",
    "\n",
    "# Compute predictions on training data\n",
    "scores_train = w0_value + X_train @ w_value  # Scores for training data\n",
    "y_pred_train = np.sign(scores_train)         # Predicted labels for training data\n",
    "\n",
    "# Handle the case when score is zero (assign label 1 when score is zero)\n",
    "y_pred_train[scores_train == 0] = 1\n",
    "\n",
    "# Calculate error rate on training data\n",
    "calculate_error_rate(y_pred_train, y_train, \"Training\")\n",
    "\n",
    "# Compute predictions on test data\n",
    "scores_test = w0_value + X_test @ w_value    # Scores for test data\n",
    "y_pred_test = np.sign(scores_test)           # Predicted labels for test data\n",
    "\n",
    "# Handle the case when score is zero (assign label 1 when score is zero)\n",
    "y_pred_test[scores_test == 0] = 1\n",
    "\n",
    "# Calculate error rate on test data\n",
    "calculate_error_rate(y_pred_test, y_test, \"Test\")\n",
    "\n",
    "# Generate adversarial examples for the test set\n",
    "X_test_adv = compute_perturbation(X_test, y_test, w_value, P)\n",
    "\n",
    "# Compute predictions on adversarial test data\n",
    "scores_test_adv = w0_value + X_test_adv @ w_value\n",
    "y_pred_test_adv = np.sign(scores_test_adv)\n",
    "\n",
    "# Handle the case when score is zero (assign label 1 when score is zero)\n",
    "y_pred_test_adv[scores_test_adv == 0] = 1\n",
    "\n",
    "# Calculate error rate on adversarial test data\n",
    "calculate_error_rate(y_pred_test_adv, y_test, \"Adversarial Test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
